{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>the 4 202 353 bp genome of the alkaliphilic ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id class                                           abstract\n",
       "0   1     B  the 4 202 353 bp genome of the alkaliphilic ba..."
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import re\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load in the training set .csv\n",
    "training_set = pd.read_csv(\"trg.csv\")\n",
    "text_df = training_set\n",
    "\n",
    "training_set.head(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in range(0, text_df.shape[0]):\n",
    "    abstract = text_df[\"abstract\"][i]\n",
    "    words_abstract = abstract.split(\" \") # Split when there is a space\n",
    "    words.extend(words_abstract)\n",
    "    \n",
    "# words = list(np.unique(np.sort(words)))\n",
    "words = list(np.unique(words))\n",
    "    \n",
    "for i in range(len(words)):#removes the single quotes from these things \"'hello'\"\n",
    "    words[i] = words[i].replace(\"'\",\"\")\n",
    "\n",
    "while('' in words): #removes empty strings ''.\n",
    "    words.remove('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Bellow is the removing empty strings and removing, single quotes and rgex testing from yesterday\n",
    "# import copy\n",
    "# import re\n",
    "\n",
    "# alist = [\"'hello'\",\"'bob'\", 'hrllo','11', '11-11','-x','111111','11-hydroxy-thc', '14c-labeled' , '1751377-bp', '17alpha-hydroxylase', '26-kb', '34-45-dioxygenase', '35-fold', '-11','11-']\n",
    "\n",
    "\n",
    "# for i in range(len(alist)):\n",
    "#     alist[i] = alist[i].replace(\"'\",\"\")\n",
    "    \n",
    "# while('' in alist) : \n",
    "#     alist.remove('')\n",
    "    \n",
    "# #regex_expression = \"^\\d+-|\\d+|\\d+-\\d+$\"\n",
    "# regex_expression = \"^[-]?\\d+[-]?\\d*$\"\n",
    "# #regex_expression2 = \"^\\d+$\"\n",
    "# for i in alist:\n",
    "#     m = re.search(regex_expression, i)\n",
    "#     print(i, m)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"---------------------------------------------\")\n",
    "\n",
    "# print(alist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####The bellow is the removing numbers test###\n",
    "\n",
    "regex = re.compile(\"^[-]?\\d+[-]?\\d*$\")\n",
    "\n",
    "filtered = [i for i in words if not regex.search(i)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>the 4 202 353 bp genome of the alkaliphilic ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id class                                           abstract\n",
       "0   1     B  the 4 202 353 bp genome of the alkaliphilic ba..."
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load in the training set .csv\n",
    "training_set = pd.read_csv(\"trg.csv\")\n",
    "text_df = training_set\n",
    "\n",
    "training_set.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of_dictionary(initialized_dictionary):\n",
    "\n",
    "\n",
    "    classes = []\n",
    "    training_set = pd.read_csv(\"trg.csv\")\n",
    "    text_df = training_set\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        the_class = text_df[\"class\"][i]\n",
    "        all_the_classes = the_class.split() # Split when there is a space\n",
    "        classes.extend(all_the_classes)\n",
    "    \n",
    "    number_of_instances_in_the_list = len(classes) #this can also be the number of instances in our training set.\n",
    "    \n",
    "    for i in range(0,number_of_instances_in_the_list):\n",
    "        the_current_abstract = text_df[\"abstract\"][i]\n",
    "        splitting_the_abstract = the_current_abstract.split() # Split when there is a space\n",
    "        for a_word in splitting_the_abstract:\n",
    "            if (a_word in initialized_dictionary[classes[i]]):\n",
    "                initialized_dictionary[classes[i]][a_word] = initialized_dictionary[classes[i]][a_word] + 1\n",
    "                \n",
    "    \n",
    "    frequency_dictionary = initialized_dictionary\n",
    "    \n",
    "    #print(frequency_dictionary)\n",
    "    return frequency_dictionary\n",
    "    \n",
    "    ######IT RETURNS THIS {'A': {'the': 18145, 'their': 359, 'them': 65},......}##########DONE\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_dict_of_classes():\n",
    "    \n",
    "#############################################################################\n",
    "############creates a list of all classes used in the training set###########\n",
    "#############################################################################\n",
    "    \n",
    "    list_of_classes = []\n",
    "\n",
    "    training_set = pd.read_csv(\"trg.csv\")\n",
    "    text_df = training_set\n",
    "\n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        the_class = text_df[\"class\"][i]\n",
    "        all_the_classes = the_class.split() # Split when there is a space\n",
    "        list_of_classes.extend(all_the_classes)\n",
    "\n",
    "\n",
    "    list_of_every_class = list_of_classes\n",
    "\n",
    "##############################################################################\n",
    "####################MAKES THE DICTIONARY OF THE 4Classes######################\n",
    "##############################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    dict_of_classes = {}\n",
    "    \n",
    "    ###The bellow makes this: ['B', 'A', 'E', 'V']####\n",
    "    remove_duplicates_of_classes = []\n",
    "    \n",
    "    for each_class in list_of_every_class:\n",
    "        if(each_class not in remove_duplicates_of_classes):\n",
    "            remove_duplicates_of_classes.append(each_class)\n",
    "    \n",
    "    ###Make a dictionary for the number of class rows###\n",
    "    \n",
    "    for a_single_class in remove_duplicates_of_classes:\n",
    "        dict_of_classes[a_single_class] = 0\n",
    "\n",
    "    return(dict_of_classes)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def read_classes_from_csv():\n",
    "    # Load in the training set .csv\n",
    "    class_list = []\n",
    "    \n",
    "    training_set = pd.read_csv(\"trg.csv\")\n",
    "    text_df = training_set\n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        the_class = text_df[\"class\"][i]\n",
    "        all_the_classes = the_class.split() # Split when there is a space\n",
    "        class_list.extend(all_the_classes)\n",
    "        \n",
    "    return(class_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_amount_of_classes():\n",
    "    \n",
    "    \n",
    "    dict_of_classes = create_empty_dict_of_classes()\n",
    "    \n",
    "    \n",
    "    list_of_every_class = read_classes_from_csv()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dictionary_holding_the_amount_of_classes = dict_of_classes\n",
    "    for class_of_abstract in list_of_every_class:\n",
    "        dictionary_holding_the_amount_of_classes[class_of_abstract] = dictionary_holding_the_amount_of_classes[class_of_abstract] + 1\n",
    "    #print(dictionary_holding_the_amount_of_classes)\n",
    "    return(dictionary_holding_the_amount_of_classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "def probability_of_classes():\n",
    "\n",
    "############################################################################# \n",
    "#########DICT_STORE MAKES A EMPTY DICTIONARY FOR EACH CLASS##################\n",
    "#############################################################################\n",
    "    \n",
    "    dict_store_probability_of_classes = create_empty_dict_of_classes()\n",
    "    #print(dict_store_probability_of_classes)\n",
    "    \n",
    "#############################################################################    \n",
    "############READS IN THE TRAINING DATA AND GETS ALL THE CLASSES##############\n",
    "############################################################################# \n",
    "    classes = [] \n",
    "\n",
    "    # Load in the training set .csv\n",
    "    classes = read_classes_from_csv()\n",
    "        \n",
    "\n",
    "#########The probability of Classes the classes ##########\n",
    "    number_of_classes_of_the_list = len(classes)\n",
    "    \n",
    "    ##############################################################################\n",
    "    ############put the number of classes into the dictionary#####################\n",
    "    ######## results in this {'B': 1602, 'A': 128, 'E': 2144, 'V': 126}###########\n",
    "    ##############################################################################\n",
    "    \n",
    "    for individual_class in classes:\n",
    "        dict_store_probability_of_classes[individual_class] = dict_store_probability_of_classes[individual_class] + 1\n",
    "    \n",
    "    ##############################################################################\n",
    "    ############put the probability values into the dictionary####################\n",
    "    #### results in this {'B': 0.4005, 'A': 0.032, 'E': 0.536, 'V': 0.0315}#######\n",
    "    ##############################################################################\n",
    "    \n",
    "    for key, value in dict_store_probability_of_classes.items():\n",
    "        dict_store_probability_of_classes[key] = value/number_of_classes_of_the_list\n",
    "        \n",
    "\n",
    "    return(dict_store_probability_of_classes)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_frequency_of_classes(a_frequency_dictionary):\n",
    "    \n",
    "    the_final_dict = create_empty_dict_of_classes()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for a_class,dict_value in a_frequency_dictionary.items():\n",
    "        if isinstance(dict_value,dict):\n",
    "            for key, value in dict_value.items():\n",
    "                the_final_dict[a_class] += value\n",
    "    \n",
    "    return(the_final_dict)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####using the dictionary method####\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def process_text_and_make_dictionary():\n",
    "\n",
    "    # Load in the training set .csv\n",
    "    training_set = pd.read_csv(\"trg.csv\")\n",
    "    text_df = training_set\n",
    "\n",
    "    classes = []\n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        the_class = text_df[\"class\"][i]\n",
    "        all_the_classes = the_class.split() # Split when there is a space\n",
    "        classes.extend(all_the_classes)\n",
    "\n",
    "\n",
    "### The vocab place in a list###\n",
    "\n",
    "    vocab_list = []\n",
    "    for i in range(0, text_df.shape[0]):\n",
    "\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        words_abstract = abstract.split() # Split when there is a space\n",
    "        vocab_list.extend(words_abstract)\n",
    "\n",
    "\n",
    "    # Remove duplicate words\n",
    "    vocab_list = list(np.unique(vocab_list))\n",
    "    #print(vocab_list)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ###Making a dict of a dict for the amount of words for each document####\n",
    "    \n",
    "    ###The classes list need to be squish down into the main 4 classes.####\n",
    "    \n",
    "    classes = np.array(classes)\n",
    "    classes = (np.unique(classes))\n",
    "    classes = list(classes)\n",
    "\n",
    "\n",
    "    ###now trying to create the empty dictionary###\n",
    "    final_dict = {}\n",
    "    # remember vocab_list\n",
    "\n",
    "    ###how to initalize a dict of a dict with value 0####\n",
    "\n",
    "    for each_class in classes:\n",
    "        final_dict[each_class] = {}\n",
    "        for each_word in vocab_list:\n",
    "            final_dict[each_class][each_word] = 0\n",
    "\n",
    "    return(final_dict)\n",
    "    \n",
    "    ####### Returns this {'A': {\"'\": 0, \"'87\": 0, \"'a\": 0, \"'a'\": 0}......############\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_frequency_log_part():\n",
    "    vocab_list = []\n",
    "    inverse_freq_word_dict = {}\n",
    "    total_instances = 0\n",
    "    \n",
    "    training_set = pd.read_csv(\"trg.csv\")\n",
    "    text_df = training_set\n",
    "    text_df.head()\n",
    "    \n",
    "        \n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        total_instances += 1\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        words_abstract = abstract.split() # Split when there is a space\n",
    "        vocab_list.extend(words_abstract)\n",
    "\n",
    "\n",
    "    #####Remove duplicate words#####\n",
    "    vocab_list = list(np.unique(vocab_list))\n",
    "    \n",
    "    \n",
    "    for item in vocab_list:\n",
    "        inverse_freq_word_dict[item] = 0\n",
    "        \n",
    "#####this loops through at each abstract and adds 1 to all the unique words in the dict######\n",
    "        \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        words_abstract = abstract.split() # Split for the current abstract\n",
    "        done_before = []\n",
    "        for a_word in words_abstract:\n",
    "            if(a_word in inverse_freq_word_dict and a_word not in done_before):\n",
    "                done_before.append(a_word)\n",
    "                inverse_freq_word_dict[a_word] = inverse_freq_word_dict[a_word] + 1\n",
    "    \n",
    "    for key, value in inverse_freq_word_dict.items():\n",
    "        inverse_freq_word_dict[key] = math.log(total_instances/value)\n",
    "        \n",
    "    \n",
    "    return inverse_freq_word_dict\n",
    "    \n",
    "    \n",
    "    ###############################################################################################################\n",
    "    ######### IT RETURNS THE DICTIONARY {'Auckland': 1.3862943611198906, 'Bird': 1.3862943611198906....}###########\n",
    "    ################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "####### list_of_every_class is not needed anymore#############\n",
    "def main():\n",
    "\n",
    "    list_of_probabilities = probability_of_classes() #Done\n",
    "    \n",
    "    inverse_freq_word_dict = inverse_frequency_log_part() ####1st part of inverse_dict.\n",
    "    \n",
    "    the_empty_frequenct_dict = process_text_and_make_dictionary() #### 2(a) part empty dict.\n",
    "    \n",
    "    reverse_frequency = reverse_frequency_dict(the_empty_frequenct_dict, inverse_freq_word_dict) #### 2(b) part fill the dict and the the final values.\n",
    "    \n",
    "    naive_bayes_algorithm(reverse_frequency, list_of_probabilities)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #the_empty_dictionary_of_vocab_words = process_text_and_make_dictionary()#Done\n",
    "    \n",
    "    #a_frequency_dictionary = frequency_of_dictionary(the_empty_dictionary_of_vocab_words)#Done\n",
    "    \n",
    "    #the_frequency_of_rows = dict_amount_of_classes()#Done\n",
    "\n",
    "    #occurences_of_words_for_classes = dict_frequency_of_classes(a_frequency_dictionary)#Done\n",
    "    \n",
    "    #print('PROBABILITY', list_of_probabilities)\n",
    "    #print('FIRST PRINT',the_frequency_of_rows)\n",
    "    #print('SECOND PRINT',occurences_of_words_for_classes)\n",
    "    \n",
    "    #naive_bayes_algorithm(the_frequency_of_rows, occurences_of_words_for_classes, a_frequency_dictionary, list_of_probabilities)\n",
    "    \n",
    "\n",
    "main()\n",
    "##### REMEMBER TO USE SPLIT() DON'T USE SPLIT(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_frequency_dict(the_empty_frequenct_dict, inverse_freq_word_dict):\n",
    "\n",
    "    training_set = pd.read_csv(\"trg.csv\")\n",
    "    text_df = training_set\n",
    "    text_df.head()\n",
    "    classes = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    the_weighted_sum_dict = create_empty_dict_of_classes()\n",
    "    \n",
    "    ####this gets the classes########\n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        the_class = text_df[\"class\"][i]\n",
    "        all_the_classes = the_class.split() # Split when there is a space\n",
    "        classes.extend(all_the_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####did not use this count to times, I just added the logs together.\n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        words_abstract = abstract.split() # Split when there is a space\n",
    "        for a_single_word in words_abstract:\n",
    "            the_empty_frequenct_dict[classes[i]][a_single_word] = the_empty_frequenct_dict[classes[i]][a_single_word] + 1\n",
    "    \n",
    "    ######this is what the above does#########\n",
    "    ######{'DE': {'Auckland': 0, 'Bird': 0, 'Kiwi': 1, 'Munich': 1, 'Oktobrefrest': 1, 'Sheep': 0}, 'NZ': {'Auckland': 1, 'Bird': 1, 'Kiwi': 5, 'Munich': 0, 'Oktobrefrest': 0, 'Sheep': 1}}#######\n",
    "        \n",
    "    ###############################################\n",
    "    #######The bellow adds the log together########\n",
    "    ###############################################\n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        words_abstract = abstract.split() # Split when there is a space\n",
    "        for a_word in words_abstract:\n",
    "            the_weighted_sum_dict[classes[i]] = the_weighted_sum_dict[classes[i]] + inverse_freq_word_dict[a_word]\n",
    "    \n",
    "    #print(the_weighted_sum_dict)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    ##################REPLACING THE PRIORS WITH THE FINAL STEP BELLOW#########################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    \n",
    "    final_dict = process_text_and_make_dictionary()\n",
    "    \n",
    "#     print(final_dict)\n",
    "#    print(the_empty_frequenct_dict)\n",
    "#     print(inverse_freq_word_dict)\n",
    "    #print(the_weighted_sum_dict)\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################################################################\n",
    "    ##################GETTING THE COUNT OF ALL THE UNIQUE LIST################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    vocab_list = []\n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        words_abstract = abstract.split() # Split when there is a space\n",
    "        vocab_list.extend(words_abstract)\n",
    "\n",
    "\n",
    "    #####Remove duplicate words#####\n",
    "    vocab_list = list(np.unique(vocab_list))\n",
    "    \n",
    "    number_for_the_unique_vocab = len(vocab_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    ##################GETTING THE COUNT OF ALL THE UNIQUE LIST################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        abstract = text_df[\"abstract\"][i]\n",
    "        for a_word in vocab_list:\n",
    "            #print(a_word)\n",
    "            #print(the_empty_frequenct_dict[classes[i]][a_word], \"*\", inverse_freq_word_dict[a_word], \"+\" \"1\", \"/\",  the_weighted_sum_dict[classes[i]], \"+\", number_for_the_unique_vocab)\n",
    "            final_dict[classes[i]][a_word] = (((the_empty_frequenct_dict[classes[i]][a_word] * inverse_freq_word_dict[a_word]) + 1) / (the_weighted_sum_dict[classes[i]] + number_for_the_unique_vocab))\n",
    "    #print('--------------------------------------------------')\n",
    "            \n",
    "    #print(final_dict)\n",
    "    #print('--------------------------------------------------')\n",
    "    #print(inverse_freq_word_dict)\n",
    "    return(final_dict)\n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_algorithm(nested_dict_of_class_and_word_values, list_of_class_probabilities):\n",
    "\n",
    "    \n",
    "    test_list = []\n",
    "    \n",
    "    test_set = pd.read_csv(\"tst.csv\")\n",
    "    text_df = test_set\n",
    "    \n",
    "    #####GETS THE AMOUNT OF ROWS THERE ARE IN THE TEST SET#########\n",
    "    \n",
    " \n",
    "     ######starting the naive bayes algorithm############\n",
    "    \n",
    "    naive_bayes_probability_dict = {}\n",
    "    test_class_classification_list = []\n",
    "    current_test_abstract = {}\n",
    "    output_dictionary = []\n",
    "    count = 1\n",
    "    \n",
    "    for i in range(0, text_df.shape[0]):\n",
    "        the_current_test_data = text_df[\"abstract\"][i]\n",
    "        splitting_test_data = the_current_test_data.split()\n",
    "\n",
    "        for the_class_key in list_of_class_probabilities.keys():\n",
    "            naive_bayes_probability_dict[the_class_key] = math.log(list_of_class_probabilities[the_class_key])\n",
    "#     print(naive_bayes_probability_dict)\n",
    "            current_test_abstract = {}\n",
    "            for a_split_word in splitting_test_data:\n",
    "                if(a_split_word in nested_dict_of_class_and_word_values[the_class_key] and a_split_word.isdigit() == False):\n",
    "                    if a_split_word in current_test_abstract:\n",
    "                        current_test_abstract[a_split_word] = current_test_abstract[a_split_word] + 1\n",
    "                    else:\n",
    "                        current_test_abstract[a_split_word] = 1\n",
    "        #####this is multiplying everything together#######\n",
    "            for key_word, count_value in current_test_abstract.items():\n",
    "                #print(current_test_abstract[\"the\"])\n",
    "                #print(naive_bayes_probability_dict[the_class_key])#OK\n",
    "                #print(laplace_smoothing_word_probabiltiy_dictionary[the_class_key][key_word])\n",
    "                naive_bayes_probability_dict[the_class_key] = naive_bayes_probability_dict[the_class_key] + math.log((nested_dict_of_class_and_word_values[the_class_key][key_word] ** current_test_abstract[key_word]))\n",
    "        #print(naive_bayes_probability_dict)\n",
    "            \n",
    "            #temp_list = list(naive_bayes_probability_dict.values())\n",
    "            #test_class_classification_list.append(temp_list)\n",
    "        test_class_classification_list.append(max(naive_bayes_probability_dict, key=naive_bayes_probability_dict.get))\n",
    "            \n",
    "    for item in test_class_classification_list:\n",
    "        output_dictionary.append({count: item})\n",
    "        count+=1\n",
    "    #print(output_dictionary)\n",
    "    #print(output_dictionary)\n",
    "    \n",
    "    test_set[\"class\"] = test_class_classification_list\n",
    "    test_set.drop([\"abstract\"], axis = 1).to_csv(\"tst_kaggle_isdigit_at_naive.csv\", index=False)\n",
    "    \n",
    "#remove numbers from the nested dict and remove numbers from the current_test_abstracts.\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "####SUBMITTED THIS FILE LAST NIGHT MONDAY 3:39AM <=== results in the same score.\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
